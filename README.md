# Adam Paper Experiments Replication

This repository contains a complete, modular implementation to replicate all experiments from **Section 6** of the paper:

> **"Adam: A Method for Stochastic Optimization"**  
> by Diederik P. Kingma and Jimmy Lei Ba (2014)  
> [https://arxiv.org/pdf/1412.6980](https://arxiv.org/pdf/1412.6980)

## ğŸ“‹ Overview

This project replicates the following experiments:
- **Experiment 6.1**: Logistic Regression on MNIST
- **Experiment 6.2**: Multi-Layer Perceptron on MNIST
- **Experiment 6.3**: Convolutional Neural Network on CIFAR-10
- **Experiment 6.4**: Advanced Experiments and Ablation Studies

## ğŸ—ï¸ Project Structure

```
AdamSGD/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # Hyperparameters and settings
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ adam.py                     # Adam optimizer (Algorithm 1 from paper)
â”‚   â”œâ”€â”€ sgd_momentum.py             # SGD with momentum
â”‚   â”œâ”€â”€ adagrad.py                  # AdaGrad optimizer
â”‚   â””â”€â”€ rmsprop.py                  # RMSProp optimizer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_regression.py     # Logistic regression model
â”‚   â”œâ”€â”€ mlp.py                      # Multi-layer perceptron
â”‚   â””â”€â”€ cnn.py                      # Convolutional neural network
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_loaders.py            # MNIST and CIFAR-10 loaders (auto-download)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ training.py                # Training utilities
â”‚   â”œâ”€â”€ evaluation.py              # Evaluation metrics
â”‚   â””â”€â”€ plotting.py                # Visualization utilities
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp_6_1_logistic_regression.py
â”‚   â”œâ”€â”€ exp_6_2_mlp.py
â”‚   â”œâ”€â”€ exp_6_3_cnn.py
â”‚   â””â”€â”€ exp_6_4_advanced.py
â”œâ”€â”€ results/                        # Generated plots and results
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_all_experiments.py         # Master script
```

## ğŸš€ Quick Start

### 1. Setup Environment

First, activate your virtual environment and install dependencies:

```bash
cd AdamSGD
source adamVenv/bin/activate  # Already created
pip install -r requirements.txt
```

### 2. Run All Experiments

To run all experiments at once:

```bash
python run_all_experiments.py
```

### 3. Run Individual Experiments

You can also run experiments individually:

```bash
# Experiment 6.1: Logistic Regression on MNIST
python experiments/exp_6_1_logistic_regression.py

# Experiment 6.2: MLP on MNIST
python experiments/exp_6_2_mlp.py

# Experiment 6.3: CNN on CIFAR-10
python experiments/exp_6_3_cnn.py

# Experiment 6.4: Advanced experiments and ablations
python experiments/exp_6_4_advanced.py
```

### 4. Run Specific Experiments

```bash
# Run only experiments 6.1 and 6.2
python run_all_experiments.py --experiments 6.1 6.2

# Run with custom config
python run_all_experiments.py --config path/to/config.yaml
```

## âš™ï¸ Configuration

Edit `config/config.yaml` to customize experiments:

### Key Settings

**Mode**: Switch between `fast` (quick iteration) and `full` (paper-matching) mode
```yaml
mode: 'fast'  # or 'full'
```

**Device**: Configure for Apple Silicon (MPS), CUDA, or CPU
```yaml
device:
  use_mps: true   # Apple Silicon GPU
  use_cuda: false
```

**Optimizer Hyperparameters**: 
- Adam: Î±=0.001, Î²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8 (paper defaults)
- Can customize learning rates for each optimizer

**Training Settings**:
- Epochs (fast mode): LogReg=10, MLP=20, CNN=30
- Epochs (full mode): LogReg=100, MLP=100, CNN=200
- Batch size: 128 (configurable)

## ğŸ“Š Results and Plots

After running experiments, results are saved in:

```
results/
â”œâ”€â”€ exp_6_1_logistic_regression/
â”‚   â”œâ”€â”€ comparison_train_loss.png
â”‚   â”œâ”€â”€ comparison_val_loss.png
â”‚   â”œâ”€â”€ comparison_train_acc.png
â”‚   â”œâ”€â”€ comparison_val_acc.png
â”‚   â””â”€â”€ [optimizer]_curves.png (for each optimizer)
â”œâ”€â”€ exp_6_2_mlp/
â”‚   â””â”€â”€ [similar structure]
â”œâ”€â”€ exp_6_3_cnn/
â”‚   â””â”€â”€ [similar structure]
â””â”€â”€ exp_6_4_advanced/
    â”œâ”€â”€ lr_ablation_val_loss.png
    â”œâ”€â”€ lr_ablation_val_acc.png
    â”œâ”€â”€ beta_ablation_val_loss.png
    â”œâ”€â”€ deep_network_train_loss.png
    â””â”€â”€ deep_network_val_acc.png
```

## ğŸ§ª Experiments Details

### Experiment 6.1: Logistic Regression on MNIST
- **Model**: Single linear layer (784 â†’ 10)
- **Optimizers**: Adam, SGD, SGD+Momentum, AdaGrad, RMSProp
- **Dataset**: MNIST (60k train, 10k test)
- **Objective**: Compare optimizer convergence on simple convex problem

### Experiment 6.2: Multi-Layer Perceptron on MNIST
- **Model**: 2 hidden layers (784 â†’ 1000 â†’ 1000 â†’ 10) with ReLU and dropout
- **Optimizers**: Adam, SGD+Momentum, AdaGrad, RMSProp
- **Dataset**: MNIST
- **Objective**: Compare on deeper non-convex problem

### Experiment 6.3: CNN on CIFAR-10
- **Model**: 3 conv blocks + 3 FC layers with dropout
- **Optimizers**: Adam, SGD+Momentum, AdaGrad, RMSProp
- **Dataset**: CIFAR-10 (50k train, 10k test)
- **Objective**: Compare on vision task with spatial structure

### Experiment 6.4: Advanced Experiments
- **Learning Rate Ablation**: Test Adam with different learning rates
- **Beta Parameter Ablation**: Test different Î²â‚ and Î²â‚‚ values
- **Deep Network**: 3 hidden layers comparison
- **Objective**: Sensitivity analysis and robustness testing

## ğŸ”¬ Implementation Details

### Adam Optimizer
Implements Algorithm 1 from the paper:

```python
m_t = Î²â‚ Â· m_{t-1} + (1 - Î²â‚) Â· g_t
v_t = Î²â‚‚ Â· v_{t-1} + (1 - Î²â‚‚) Â· g_tÂ²
mÌ‚_t = m_t / (1 - Î²â‚^t)
vÌ‚_t = v_t / (1 - Î²â‚‚^t)
Î¸_t = Î¸_{t-1} - Î± Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

### Key Features
- âœ… Modular, clean code structure
- âœ… MPS backend support for Apple Silicon
- âœ… Automatic dataset downloading
- âœ… Configurable hyperparameters
- âœ… Comprehensive plotting matching paper figures
- âœ… Progress bars and logging
- âœ… Fast and full experiment modes

## ğŸ“ˆ Expected Results

Based on the paper, you should observe:
- **Adam** converges faster and achieves better final performance
- **SGD** requires careful learning rate tuning
- **AdaGrad** works well initially but learning rate decays too aggressively
- **RMSProp** performs well but Adam's bias correction helps
- **Learning rate ablation** shows Adam is robust to different learning rates

## ğŸ–¥ï¸ System Requirements

- Python 3.9+
- PyTorch 2.0+
- 8GB+ RAM
- GPU recommended but not required
  - Apple Silicon: Uses MPS backend
  - NVIDIA: Set `use_cuda: true` in config
  - CPU: Falls back automatically

## ğŸ“ Notes

- **Fast mode** is recommended for initial testing (10-30 minutes total)
- **Full mode** matches paper settings but takes longer (2-6 hours depending on hardware)
- Datasets are automatically downloaded to `data/` directory
- All random seeds are fixed for reproducibility (seed=42)

## ğŸ” Troubleshooting

**Issue**: MPS not available
```bash
# Check PyTorch MPS support
python -c "import torch; print(torch.backends.mps.is_available())"
```

**Issue**: Out of memory
- Reduce batch size in config.yaml
- Use CPU instead of MPS/CUDA
- Run experiments individually instead of all at once

**Issue**: Slow training
- Ensure MPS backend is enabled in config
- Use fast mode for quicker iteration
- Reduce number of epochs

## ğŸ“š References

1. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
2. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. JMLR.
3. Tieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning.

## ğŸ“„ License

This implementation is for educational and research purposes, replicating the experiments from the Adam paper.

## ğŸ™ Acknowledgments

This implementation replicates the experiments from:
- Paper: "Adam: A Method for Stochastic Optimization" by Kingma & Ba
- Paper URL: https://arxiv.org/pdf/1412.6980

---

**Happy Experimenting! ğŸš€**
